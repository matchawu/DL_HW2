{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# # for imbalanced dataset\n",
    "# from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision.utils import save_image\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "number of train data: 19395\nnumber of test data: 2156\n"
    }
   ],
   "source": [
    "data_path = '../anime-faces/data/'\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64,64)), # resize images to same size\n",
    "        torchvision.transforms.ToTensor(), # image to tensor\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        # mean=[.5,.5,.5],std=[.5,.5,.5]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# dataset\n",
    "dataset = torchvision.datasets.ImageFolder(\n",
    "    root=data_path,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# train test split\n",
    "num_data = len(dataset)\n",
    "ratio = .9\n",
    "# print(num_data)\n",
    "# print(int(num_data*0.8), num_data-int(num_data*0.8))\n",
    "trainSet, testSet = torch.utils.data.random_split(dataset, [int(num_data*ratio), num_data-int(num_data*ratio)])\n",
    "print(\"number of train data:\", len(trainSet))\n",
    "print(\"number of test data:\", len(testSet))\n",
    "\n",
    "# dataloader\n",
    "trainLoader = torch.utils.data.DataLoader(\n",
    "    trainSet,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    shuffle=True\n",
    ")\n",
    "testLoader = torch.utils.data.DataLoader(\n",
    "    testSet,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    shuffle=True\n",
    ")\n",
    "# print(train_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epochs:  15\nBatch size:  32\nLatent code:  32\nSave log interval:  50\nBatch nums:  606.09375\nImage size:  12288\n"
    }
   ],
   "source": [
    "'''\n",
    "印出設定\n",
    "'''\n",
    "def printARGS():\n",
    "    epochs = 15\n",
    "    batch_size = 32# num_workers = 2   # num_workers\n",
    "    latent_code = 32   \n",
    "    save_interval = 50\n",
    "    batch = len(trainSet) / batch_size\n",
    "    image_size = int(64 * 64 * 3)\n",
    "\n",
    "    print(\"Epochs: \", epochs)\n",
    "    print(\"Batch size: \", batch_size)\n",
    "    print(\"Latent code: \", latent_code)\n",
    "    print(\"Save log interval: \", save_interval)\n",
    "    print(\"Batch nums: \", batch)\n",
    "    print(\"Image size: \", image_size)\n",
    "\n",
    "printARGS()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(64, 64)\n"
    }
   ],
   "source": [
    "# '''\n",
    "# 試著印出圖片\n",
    "# 並確認圖片大小\n",
    "# '''\n",
    "# from matplotlib.pyplot import imshow\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import glob\n",
    "\n",
    "# # %matplotlib inline\n",
    "\n",
    "# image_list = []\n",
    "# image_size = []\n",
    "# for filename in glob.glob('../anime-faces/data/*.png'):\n",
    "#     img = Image.open(filename)\n",
    "#     image_list.append(img)\n",
    "#     image_size.append(img.size)\n",
    "#     img.load()\n",
    "# # img = Image.open('../anime-faces/data/1.png', 'r')\n",
    "# # imshow(np.asarray(img))\n",
    "# # print(img.size)\n",
    "# def unique(list1):\n",
    "#     unique_list = []\n",
    "#     for x in list1:\n",
    "#         if x not in unique_list: \n",
    "#             unique_list.append(x)\n",
    "#     for x in unique_list: \n",
    "#         print(x)\n",
    "# unique(image_size) # size 皆是 64*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\n公式如下: \\nnew_height = new_width = (W — F + 1) / S （结果向上取整數，假設算出來結果是4.5，取5）\\n(原本 - filter + 1) / stride\\n\\n例子\\nfilter 3x3, stride=1, 卷積後的大小: (10–3+1)/1=8\\nfilter 3x3, stride=2, 卷積後的大小: (10–3+1)/2=4\\n'"
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "'''\n",
    "公式如下: \n",
    "new_height = new_width = (W — F + 1) / S （结果向上取整數，假設算出來結果是4.5，取5）\n",
    "(原本 - filter + 1) / stride\n",
    "\n",
    "例子\n",
    "filter 3x3, stride=1, 卷積後的大小: (10–3+1)/1=8\n",
    "filter 3x3, stride=2, 卷積後的大小: (10–3+1)/2=4\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN VAE\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=1024):\n",
    "        return input.view(input.size(0), size, 1, 1)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=1024, z_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 32, 31, 31]           1,568\n       BatchNorm2d-2           [-1, 32, 31, 31]              64\n              ReLU-3           [-1, 32, 31, 31]               0\n            Conv2d-4           [-1, 64, 14, 14]          32,832\n       BatchNorm2d-5           [-1, 64, 14, 14]             128\n              ReLU-6           [-1, 64, 14, 14]               0\n            Conv2d-7            [-1, 128, 6, 6]         131,200\n       BatchNorm2d-8            [-1, 128, 6, 6]             256\n              ReLU-9            [-1, 128, 6, 6]               0\n           Conv2d-10            [-1, 256, 2, 2]         524,544\n      BatchNorm2d-11            [-1, 256, 2, 2]             512\n             ReLU-12            [-1, 256, 2, 2]               0\n          Flatten-13                 [-1, 1024]               0\n           Linear-14                   [-1, 32]          32,800\n           Linear-15                   [-1, 32]          32,800\n           Linear-16                 [-1, 1024]          33,792\n        UnFlatten-17           [-1, 1024, 1, 1]               0\n  ConvTranspose2d-18            [-1, 128, 5, 5]       3,276,928\n             ReLU-19            [-1, 128, 5, 5]               0\n  ConvTranspose2d-20           [-1, 64, 13, 13]         204,864\n             ReLU-21           [-1, 64, 13, 13]               0\n  ConvTranspose2d-22           [-1, 32, 30, 30]          73,760\n             ReLU-23           [-1, 32, 30, 30]               0\n  ConvTranspose2d-24            [-1, 3, 64, 64]           3,459\n          Sigmoid-25            [-1, 3, 64, 64]               0\n================================================================\nTotal params: 4,349,507\nTrainable params: 4,349,507\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.05\nForward/backward pass size (MB): 1.98\nParams size (MB): 16.59\nEstimated Total Size (MB): 18.62\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "vae = VAE()\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "'''\n",
    "loss function\n",
    "'''\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD, BCE, KLD\n",
    "'''\n",
    "model summary\n",
    "'''\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "summary(vae, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(x):\n",
    "    recon_x, _, _ = vae(x/255.)\n",
    "    return torch.cat([x, recon_x/255.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "462 BCE: 201.400 KL: 0.062\nEpoch[38/50] Loss: 212.408 BCE: 212.347 KL: 0.061\nEpoch[38/50] Loss: 206.483 BCE: 206.422 KL: 0.060\nEpoch[38/50] Loss: 205.170 BCE: 205.108 KL: 0.062\nEpoch[38/50] Loss: 209.110 BCE: 209.047 KL: 0.064\nEpoch[38/50] Loss: 208.812 BCE: 208.751 KL: 0.062\nEpoch[38/50] Loss: 210.779 BCE: 210.719 KL: 0.061\nEpoch[38/50] Loss: 210.124 BCE: 210.062 KL: 0.061\nEpoch[38/50] Loss: 211.719 BCE: 211.659 KL: 0.060\nEpoch[38/50] Loss: 207.933 BCE: 207.872 KL: 0.061\nEpoch[38/50] Training Loss: 210.781\nEpoch[38/50] Testing Loss: 211.802\nEpoch[39/50] Loss: 213.336 BCE: 213.273 KL: 0.062\nEpoch[39/50] Loss: 208.070 BCE: 208.008 KL: 0.062\nEpoch[39/50] Loss: 211.888 BCE: 211.826 KL: 0.062\nEpoch[39/50] Loss: 208.416 BCE: 208.356 KL: 0.060\nEpoch[39/50] Loss: 213.511 BCE: 213.449 KL: 0.061\nEpoch[39/50] Loss: 209.251 BCE: 209.189 KL: 0.061\nEpoch[39/50] Loss: 206.539 BCE: 206.477 KL: 0.062\nEpoch[39/50] Loss: 216.532 BCE: 216.472 KL: 0.060\nEpoch[39/50] Loss: 212.686 BCE: 212.625 KL: 0.061\nEpoch[39/50] Loss: 209.263 BCE: 209.202 KL: 0.061\nEpoch[39/50] Loss: 209.572 BCE: 209.511 KL: 0.061\nEpoch[39/50] Loss: 212.783 BCE: 212.723 KL: 0.060\nEpoch[39/50] Loss: 209.917 BCE: 209.857 KL: 0.061\nEpoch[39/50] Loss: 211.667 BCE: 211.606 KL: 0.061\nEpoch[39/50] Loss: 214.262 BCE: 214.202 KL: 0.060\nEpoch[39/50] Loss: 208.177 BCE: 208.117 KL: 0.060\nEpoch[39/50] Loss: 213.634 BCE: 213.574 KL: 0.060\nEpoch[39/50] Loss: 211.819 BCE: 211.758 KL: 0.061\nEpoch[39/50] Loss: 209.704 BCE: 209.646 KL: 0.059\nEpoch[39/50] Loss: 207.380 BCE: 207.321 KL: 0.059\nEpoch[39/50] Loss: 208.872 BCE: 208.813 KL: 0.059\nEpoch[39/50] Loss: 211.478 BCE: 211.419 KL: 0.059\nEpoch[39/50] Loss: 211.445 BCE: 211.385 KL: 0.059\nEpoch[39/50] Loss: 213.573 BCE: 213.514 KL: 0.059\nEpoch[39/50] Loss: 206.247 BCE: 206.188 KL: 0.059\nEpoch[39/50] Loss: 209.209 BCE: 209.150 KL: 0.059\nEpoch[39/50] Loss: 205.911 BCE: 205.851 KL: 0.060\nEpoch[39/50] Loss: 210.221 BCE: 210.163 KL: 0.058\nEpoch[39/50] Loss: 211.498 BCE: 211.439 KL: 0.059\nEpoch[39/50] Loss: 214.757 BCE: 214.697 KL: 0.060\nEpoch[39/50] Loss: 210.643 BCE: 210.583 KL: 0.059\nEpoch[39/50] Training Loss: 210.790\nEpoch[39/50] Testing Loss: 211.852\nEpoch[40/50] Loss: 202.215 BCE: 202.156 KL: 0.059\nEpoch[40/50] Loss: 205.408 BCE: 205.350 KL: 0.059\nEpoch[40/50] Loss: 208.776 BCE: 208.717 KL: 0.059\nEpoch[40/50] Loss: 211.268 BCE: 211.209 KL: 0.058\nEpoch[40/50] Loss: 212.644 BCE: 212.586 KL: 0.058\nEpoch[40/50] Loss: 204.371 BCE: 204.312 KL: 0.059\nEpoch[40/50] Loss: 214.293 BCE: 214.236 KL: 0.057\nEpoch[40/50] Loss: 204.524 BCE: 204.466 KL: 0.058\nEpoch[40/50] Loss: 211.855 BCE: 211.797 KL: 0.058\nEpoch[40/50] Loss: 216.229 BCE: 216.171 KL: 0.058\nEpoch[40/50] Loss: 216.097 BCE: 216.039 KL: 0.058\nEpoch[40/50] Loss: 214.860 BCE: 214.802 KL: 0.058\nEpoch[40/50] Loss: 205.229 BCE: 205.170 KL: 0.059\nEpoch[40/50] Loss: 206.380 BCE: 206.320 KL: 0.059\nEpoch[40/50] Loss: 210.656 BCE: 210.595 KL: 0.060\nEpoch[40/50] Loss: 200.058 BCE: 199.999 KL: 0.059\nEpoch[40/50] Loss: 207.522 BCE: 207.463 KL: 0.059\nEpoch[40/50] Loss: 208.629 BCE: 208.571 KL: 0.058\nEpoch[40/50] Loss: 212.772 BCE: 212.714 KL: 0.058\nEpoch[40/50] Loss: 207.639 BCE: 207.581 KL: 0.059\nEpoch[40/50] Loss: 214.491 BCE: 214.435 KL: 0.056\nEpoch[40/50] Loss: 212.999 BCE: 212.940 KL: 0.059\nEpoch[40/50] Loss: 206.184 BCE: 206.126 KL: 0.058\nEpoch[40/50] Loss: 213.837 BCE: 213.780 KL: 0.057\nEpoch[40/50] Loss: 214.901 BCE: 214.844 KL: 0.058\nEpoch[40/50] Loss: 210.805 BCE: 210.748 KL: 0.057\nEpoch[40/50] Loss: 215.198 BCE: 215.141 KL: 0.057\nEpoch[40/50] Loss: 212.354 BCE: 212.297 KL: 0.058\nEpoch[40/50] Loss: 204.901 BCE: 204.843 KL: 0.058\nEpoch[40/50] Loss: 210.025 BCE: 209.966 KL: 0.059\nEpoch[40/50] Loss: 208.531 BCE: 208.472 KL: 0.059\nEpoch[40/50] Training Loss: 210.817\nEpoch[40/50] Testing Loss: 211.449\nEpoch[41/50] Loss: 215.201 BCE: 215.144 KL: 0.057\nEpoch[41/50] Loss: 209.248 BCE: 209.191 KL: 0.058\nEpoch[41/50] Loss: 204.724 BCE: 204.665 KL: 0.058\nEpoch[41/50] Loss: 209.721 BCE: 209.663 KL: 0.058\nEpoch[41/50] Loss: 207.427 BCE: 207.369 KL: 0.058\nEpoch[41/50] Loss: 208.589 BCE: 208.531 KL: 0.058\nEpoch[41/50] Loss: 205.904 BCE: 205.847 KL: 0.057\nEpoch[41/50] Loss: 207.670 BCE: 207.613 KL: 0.058\nEpoch[41/50] Loss: 211.732 BCE: 211.675 KL: 0.057\nEpoch[41/50] Loss: 211.955 BCE: 211.898 KL: 0.057\nEpoch[41/50] Loss: 204.484 BCE: 204.425 KL: 0.059\nEpoch[41/50] Loss: 220.350 BCE: 220.292 KL: 0.058\nEpoch[41/50] Loss: 212.553 BCE: 212.496 KL: 0.056\nEpoch[41/50] Loss: 212.700 BCE: 212.644 KL: 0.056\nEpoch[41/50] Loss: 210.947 BCE: 210.891 KL: 0.056\nEpoch[41/50] Loss: 216.557 BCE: 216.499 KL: 0.059\nEpoch[41/50] Loss: 212.719 BCE: 212.663 KL: 0.056\nEpoch[41/50] Loss: 218.048 BCE: 217.992 KL: 0.056\nEpoch[41/50] Loss: 207.910 BCE: 207.853 KL: 0.057\nEpoch[41/50] Loss: 209.627 BCE: 209.571 KL: 0.056\nEpoch[41/50] Loss: 212.641 BCE: 212.585 KL: 0.056\nEpoch[41/50] Loss: 219.184 BCE: 219.128 KL: 0.056\nEpoch[41/50] Loss: 209.777 BCE: 209.721 KL: 0.056\nEpoch[41/50] Loss: 209.075 BCE: 209.017 KL: 0.057\nEpoch[41/50] Loss: 209.151 BCE: 209.094 KL: 0.057\nEpoch[41/50] Loss: 203.143 BCE: 203.086 KL: 0.057\nEpoch[41/50] Loss: 211.416 BCE: 211.360 KL: 0.056\nEpoch[41/50] Loss: 219.779 BCE: 219.723 KL: 0.055\nEpoch[41/50] Loss: 212.971 BCE: 212.915 KL: 0.056\nEpoch[41/50] Loss: 211.116 BCE: 211.059 KL: 0.057\nEpoch[41/50] Loss: 205.159 BCE: 205.102 KL: 0.057\nEpoch[41/50] Training Loss: 210.727\nEpoch[41/50] Testing Loss: 211.035\nEpoch[42/50] Loss: 207.499 BCE: 207.443 KL: 0.057\nEpoch[42/50] Loss: 199.858 BCE: 199.800 KL: 0.058\nEpoch[42/50] Loss: 212.803 BCE: 212.748 KL: 0.055\nEpoch[42/50] Loss: 207.765 BCE: 207.710 KL: 0.055\nEpoch[42/50] Loss: 206.606 BCE: 206.549 KL: 0.057\nEpoch[42/50] Loss: 209.362 BCE: 209.305 KL: 0.057\nEpoch[42/50] Loss: 211.772 BCE: 211.716 KL: 0.056\nEpoch[42/50] Loss: 207.147 BCE: 207.091 KL: 0.056\nEpoch[42/50] Loss: 206.776 BCE: 206.720 KL: 0.056\nEpoch[42/50] Loss: 214.673 BCE: 214.617 KL: 0.057\nEpoch[42/50] Loss: 212.527 BCE: 212.472 KL: 0.055\nEpoch[42/50] Loss: 208.826 BCE: 208.770 KL: 0.057\nEpoch[42/50] Loss: 203.718 BCE: 203.658 KL: 0.061\nEpoch[42/50] Loss: 214.130 BCE: 214.074 KL: 0.055\nEpoch[42/50] Loss: 212.327 BCE: 212.271 KL: 0.056\nEpoch[42/50] Loss: 207.376 BCE: 207.320 KL: 0.056\nEpoch[42/50] Loss: 214.112 BCE: 214.057 KL: 0.055\nEpoch[42/50] Loss: 209.031 BCE: 208.975 KL: 0.056\nEpoch[42/50] Loss: 215.312 BCE: 215.256 KL: 0.056\nEpoch[42/50] Loss: 212.752 BCE: 212.696 KL: 0.056\nEpoch[42/50] Loss: 210.646 BCE: 210.591 KL: 0.055\nEpoch[42/50] Loss: 210.893 BCE: 210.838 KL: 0.055\nEpoch[42/50] Loss: 212.579 BCE: 212.525 KL: 0.054\nEpoch[42/50] Loss: 215.160 BCE: 215.105 KL: 0.055\nEpoch[42/50] Loss: 208.763 BCE: 208.707 KL: 0.056\nEpoch[42/50] Loss: 213.316 BCE: 213.261 KL: 0.055\nEpoch[42/50] Loss: 210.620 BCE: 210.565 KL: 0.055\nEpoch[42/50] Loss: 206.759 BCE: 206.704 KL: 0.055\nEpoch[42/50] Loss: 206.327 BCE: 206.272 KL: 0.055\nEpoch[42/50] Loss: 211.453 BCE: 211.397 KL: 0.055\nEpoch[42/50] Loss: 212.588 BCE: 212.532 KL: 0.056\nEpoch[42/50] Training Loss: 210.682\nEpoch[42/50] Testing Loss: 211.003\nEpoch[43/50] Loss: 208.222 BCE: 208.165 KL: 0.056\nEpoch[43/50] Loss: 214.801 BCE: 214.747 KL: 0.055\nEpoch[43/50] Loss: 217.870 BCE: 217.817 KL: 0.054\nEpoch[43/50] Loss: 213.875 BCE: 213.820 KL: 0.054\nEpoch[43/50] Loss: 211.475 BCE: 211.421 KL: 0.054\nEpoch[43/50] Loss: 208.865 BCE: 208.811 KL: 0.055\nEpoch[43/50] Loss: 207.716 BCE: 207.661 KL: 0.055\nEpoch[43/50] Loss: 208.867 BCE: 208.813 KL: 0.054\nEpoch[43/50] Loss: 210.870 BCE: 210.815 KL: 0.055\nEpoch[43/50] Loss: 209.485 BCE: 209.427 KL: 0.058\nEpoch[43/50] Loss: 201.679 BCE: 201.625 KL: 0.055\nEpoch[43/50] Loss: 208.796 BCE: 208.741 KL: 0.054\nEpoch[43/50] Loss: 220.143 BCE: 220.089 KL: 0.054\nEpoch[43/50] Loss: 208.125 BCE: 208.070 KL: 0.055\nEpoch[43/50] Loss: 208.759 BCE: 208.703 KL: 0.056\nEpoch[43/50] Loss: 211.002 BCE: 210.947 KL: 0.055\nEpoch[43/50] Loss: 210.369 BCE: 210.315 KL: 0.055\nEpoch[43/50] Loss: 207.444 BCE: 207.389 KL: 0.055\nEpoch[43/50] Loss: 208.462 BCE: 208.408 KL: 0.054\nEpoch[43/50] Loss: 217.155 BCE: 217.101 KL: 0.054\nEpoch[43/50] Loss: 211.740 BCE: 211.686 KL: 0.054\nEpoch[43/50] Loss: 210.566 BCE: 210.512 KL: 0.054\nEpoch[43/50] Loss: 218.313 BCE: 218.260 KL: 0.052\nEpoch[43/50] Loss: 214.274 BCE: 214.220 KL: 0.055\nEpoch[43/50] Loss: 207.975 BCE: 207.921 KL: 0.054\nEpoch[43/50] Loss: 208.670 BCE: 208.614 KL: 0.056\nEpoch[43/50] Loss: 209.546 BCE: 209.493 KL: 0.053\nEpoch[43/50] Loss: 213.177 BCE: 213.123 KL: 0.054\nEpoch[43/50] Loss: 205.771 BCE: 205.716 KL: 0.055\nEpoch[43/50] Loss: 207.134 BCE: 207.080 KL: 0.054\nEpoch[43/50] Loss: 213.853 BCE: 213.799 KL: 0.054\nEpoch[43/50] Training Loss: 210.684\nEpoch[43/50] Testing Loss: 211.066\nEpoch[44/50] Loss: 209.040 BCE: 208.985 KL: 0.055\nEpoch[44/50] Loss: 214.647 BCE: 214.594 KL: 0.053\nEpoch[44/50] Loss: 212.463 BCE: 212.410 KL: 0.053\nEpoch[44/50] Loss: 218.171 BCE: 218.118 KL: 0.054\nEpoch[44/50] Loss: 211.047 BCE: 210.993 KL: 0.054\nEpoch[44/50] Loss: 214.622 BCE: 214.569 KL: 0.053\nEpoch[44/50] Loss: 209.922 BCE: 209.868 KL: 0.054\nEpoch[44/50] Loss: 216.168 BCE: 216.115 KL: 0.052\nEpoch[44/50] Loss: 217.878 BCE: 217.826 KL: 0.052\nEpoch[44/50] Loss: 202.395 BCE: 202.341 KL: 0.054\nEpoch[44/50] Loss: 209.414 BCE: 209.360 KL: 0.054\nEpoch[44/50] Loss: 215.414 BCE: 215.360 KL: 0.053\nEpoch[44/50] Loss: 201.627 BCE: 201.573 KL: 0.054\nEpoch[44/50] Loss: 206.106 BCE: 206.053 KL: 0.054\nEpoch[44/50] Loss: 215.026 BCE: 214.972 KL: 0.054\nEpoch[44/50] Loss: 209.556 BCE: 209.502 KL: 0.054\nEpoch[44/50] Loss: 210.336 BCE: 210.282 KL: 0.054\nEpoch[44/50] Loss: 216.713 BCE: 216.657 KL: 0.055\nEpoch[44/50] Loss: 209.531 BCE: 209.477 KL: 0.054\nEpoch[44/50] Loss: 207.529 BCE: 207.475 KL: 0.054\nEpoch[44/50] Loss: 210.481 BCE: 210.428 KL: 0.053\nEpoch[44/50] Loss: 204.500 BCE: 204.447 KL: 0.054\nEpoch[44/50] Loss: 205.650 BCE: 205.596 KL: 0.054\nEpoch[44/50] Loss: 216.929 BCE: 216.877 KL: 0.052\nEpoch[44/50] Loss: 207.645 BCE: 207.591 KL: 0.053\nEpoch[44/50] Loss: 208.472 BCE: 208.419 KL: 0.053\nEpoch[44/50] Loss: 207.410 BCE: 207.356 KL: 0.054\nEpoch[44/50] Loss: 215.810 BCE: 215.757 KL: 0.054\nEpoch[44/50] Loss: 205.594 BCE: 205.540 KL: 0.054\nEpoch[44/50] Loss: 211.591 BCE: 211.535 KL: 0.055\nEpoch[44/50] Loss: 202.564 BCE: 202.508 KL: 0.055\nEpoch[44/50] Training Loss: 210.634\nEpoch[44/50] Testing Loss: 211.041\nEpoch[45/50] Loss: 213.133 BCE: 213.079 KL: 0.054\nEpoch[45/50] Loss: 211.965 BCE: 211.913 KL: 0.053\nEpoch[45/50] Loss: 209.874 BCE: 209.821 KL: 0.053\nEpoch[45/50] Loss: 214.973 BCE: 214.919 KL: 0.054\nEpoch[45/50] Loss: 212.378 BCE: 212.324 KL: 0.054\nEpoch[45/50] Loss: 219.109 BCE: 219.055 KL: 0.054\nEpoch[45/50] Loss: 213.256 BCE: 213.203 KL: 0.052\nEpoch[45/50] Loss: 205.357 BCE: 205.302 KL: 0.054\nEpoch[45/50] Loss: 212.881 BCE: 212.828 KL: 0.054\nEpoch[45/50] Loss: 205.389 BCE: 205.335 KL: 0.054\nEpoch[45/50] Loss: 207.871 BCE: 207.818 KL: 0.053\nEpoch[45/50] Loss: 204.540 BCE: 204.487 KL: 0.053\nEpoch[45/50] Loss: 207.008 BCE: 206.955 KL: 0.053\nEpoch[45/50] Loss: 214.495 BCE: 214.442 KL: 0.053\nEpoch[45/50] Loss: 210.380 BCE: 210.327 KL: 0.053\nEpoch[45/50] Loss: 210.158 BCE: 210.103 KL: 0.055\nEpoch[45/50] Loss: 211.709 BCE: 211.654 KL: 0.055\nEpoch[45/50] Loss: 215.594 BCE: 215.542 KL: 0.053\nEpoch[45/50] Loss: 210.125 BCE: 210.072 KL: 0.053\nEpoch[45/50] Loss: 205.733 BCE: 205.679 KL: 0.054\nEpoch[45/50] Loss: 211.836 BCE: 211.784 KL: 0.052\nEpoch[45/50] Loss: 213.150 BCE: 213.099 KL: 0.052\nEpoch[45/50] Loss: 214.253 BCE: 214.200 KL: 0.053\nEpoch[45/50] Loss: 206.887 BCE: 206.835 KL: 0.052\nEpoch[45/50] Loss: 204.791 BCE: 204.736 KL: 0.055\nEpoch[45/50] Loss: 208.325 BCE: 208.272 KL: 0.053\nEpoch[45/50] Loss: 214.336 BCE: 214.283 KL: 0.052\nEpoch[45/50] Loss: 205.477 BCE: 205.423 KL: 0.054\nEpoch[45/50] Loss: 209.037 BCE: 208.983 KL: 0.053\nEpoch[45/50] Loss: 216.303 BCE: 216.250 KL: 0.053\nEpoch[45/50] Loss: 209.476 BCE: 209.424 KL: 0.053\nEpoch[45/50] Training Loss: 210.650\nEpoch[45/50] Testing Loss: 210.889\nEpoch[46/50] Loss: 207.815 BCE: 207.762 KL: 0.053\nEpoch[46/50] Loss: 213.633 BCE: 213.582 KL: 0.051\nEpoch[46/50] Loss: 210.808 BCE: 210.753 KL: 0.055\nEpoch[46/50] Loss: 209.357 BCE: 209.305 KL: 0.052\nEpoch[46/50] Loss: 214.768 BCE: 214.716 KL: 0.052\nEpoch[46/50] Loss: 208.345 BCE: 208.293 KL: 0.052\nEpoch[46/50] Loss: 211.574 BCE: 211.521 KL: 0.052\nEpoch[46/50] Loss: 210.185 BCE: 210.133 KL: 0.052\nEpoch[46/50] Loss: 205.183 BCE: 205.130 KL: 0.053\nEpoch[46/50] Loss: 214.528 BCE: 214.476 KL: 0.052\nEpoch[46/50] Loss: 213.528 BCE: 213.477 KL: 0.052\nEpoch[46/50] Loss: 214.043 BCE: 213.991 KL: 0.052\nEpoch[46/50] Loss: 212.875 BCE: 212.823 KL: 0.051\nEpoch[46/50] Loss: 206.667 BCE: 206.616 KL: 0.052\nEpoch[46/50] Loss: 209.483 BCE: 209.431 KL: 0.053\nEpoch[46/50] Loss: 208.171 BCE: 208.119 KL: 0.053\nEpoch[46/50] Loss: 211.784 BCE: 211.733 KL: 0.052\nEpoch[46/50] Loss: 211.112 BCE: 211.061 KL: 0.051\nEpoch[46/50] Loss: 205.993 BCE: 205.941 KL: 0.052\nEpoch[46/50] Loss: 215.727 BCE: 215.675 KL: 0.052\nEpoch[46/50] Loss: 215.019 BCE: 214.967 KL: 0.052\nEpoch[46/50] Loss: 213.461 BCE: 213.410 KL: 0.052\nEpoch[46/50] Loss: 218.972 BCE: 218.921 KL: 0.051\nEpoch[46/50] Loss: 205.219 BCE: 205.166 KL: 0.053\nEpoch[46/50] Loss: 204.863 BCE: 204.811 KL: 0.052\nEpoch[46/50] Loss: 206.513 BCE: 206.461 KL: 0.051\nEpoch[46/50] Loss: 204.853 BCE: 204.801 KL: 0.052\nEpoch[46/50] Loss: 209.917 BCE: 209.865 KL: 0.052\nEpoch[46/50] Loss: 214.928 BCE: 214.876 KL: 0.052\nEpoch[46/50] Loss: 201.594 BCE: 201.542 KL: 0.052\nEpoch[46/50] Loss: 216.646 BCE: 216.595 KL: 0.051\nEpoch[46/50] Training Loss: 210.603\nEpoch[46/50] Testing Loss: 211.231\nEpoch[47/50] Loss: 211.128 BCE: 211.077 KL: 0.050\nEpoch[47/50] Loss: 207.081 BCE: 207.030 KL: 0.051\nEpoch[47/50] Loss: 209.053 BCE: 209.003 KL: 0.051\nEpoch[47/50] Loss: 220.449 BCE: 220.396 KL: 0.053\nEpoch[47/50] Loss: 205.619 BCE: 205.567 KL: 0.051\nEpoch[47/50] Loss: 211.471 BCE: 211.420 KL: 0.051\nEpoch[47/50] Loss: 207.976 BCE: 207.924 KL: 0.052\nEpoch[47/50] Loss: 208.847 BCE: 208.796 KL: 0.051\nEpoch[47/50] Loss: 212.324 BCE: 212.273 KL: 0.051\nEpoch[47/50] Loss: 207.521 BCE: 207.469 KL: 0.051\nEpoch[47/50] Loss: 206.713 BCE: 206.662 KL: 0.052\nEpoch[47/50] Loss: 214.802 BCE: 214.751 KL: 0.050\nEpoch[47/50] Loss: 212.493 BCE: 212.444 KL: 0.049\nEpoch[47/50] Loss: 212.416 BCE: 212.365 KL: 0.051\nEpoch[47/50] Loss: 202.073 BCE: 202.021 KL: 0.052\nEpoch[47/50] Loss: 208.663 BCE: 208.613 KL: 0.050\nEpoch[47/50] Loss: 213.740 BCE: 213.690 KL: 0.050\nEpoch[47/50] Loss: 211.381 BCE: 211.331 KL: 0.050\nEpoch[47/50] Loss: 205.996 BCE: 205.945 KL: 0.051\nEpoch[47/50] Loss: 205.709 BCE: 205.658 KL: 0.051\nEpoch[47/50] Loss: 212.405 BCE: 212.355 KL: 0.050\nEpoch[47/50] Loss: 206.026 BCE: 205.975 KL: 0.051\nEpoch[47/50] Loss: 207.525 BCE: 207.474 KL: 0.052\nEpoch[47/50] Loss: 200.924 BCE: 200.873 KL: 0.051\nEpoch[47/50] Loss: 210.836 BCE: 210.785 KL: 0.051\nEpoch[47/50] Loss: 218.842 BCE: 218.792 KL: 0.050\nEpoch[47/50] Loss: 222.351 BCE: 222.302 KL: 0.049\nEpoch[47/50] Loss: 213.965 BCE: 213.915 KL: 0.050\nEpoch[47/50] Loss: 215.171 BCE: 215.121 KL: 0.050\nEpoch[47/50] Loss: 212.096 BCE: 212.046 KL: 0.050\nEpoch[47/50] Loss: 211.011 BCE: 210.960 KL: 0.050\nEpoch[47/50] Training Loss: 210.581\nEpoch[47/50] Testing Loss: 210.965\nEpoch[48/50] Loss: 209.498 BCE: 209.447 KL: 0.051\nEpoch[48/50] Loss: 210.199 BCE: 210.149 KL: 0.050\nEpoch[48/50] Loss: 210.946 BCE: 210.895 KL: 0.050\nEpoch[48/50] Loss: 210.355 BCE: 210.305 KL: 0.050\nEpoch[48/50] Loss: 214.048 BCE: 213.999 KL: 0.050\nEpoch[48/50] Loss: 207.417 BCE: 207.366 KL: 0.051\nEpoch[48/50] Loss: 206.350 BCE: 206.299 KL: 0.051\nEpoch[48/50] Loss: 214.774 BCE: 214.724 KL: 0.050\nEpoch[48/50] Loss: 212.156 BCE: 212.106 KL: 0.050\nEpoch[48/50] Loss: 204.698 BCE: 204.648 KL: 0.050\nEpoch[48/50] Loss: 209.442 BCE: 209.391 KL: 0.050\nEpoch[48/50] Loss: 213.294 BCE: 213.245 KL: 0.050\nEpoch[48/50] Loss: 213.358 BCE: 213.309 KL: 0.049\nEpoch[48/50] Loss: 215.635 BCE: 215.585 KL: 0.050\nEpoch[48/50] Loss: 210.215 BCE: 210.165 KL: 0.050\nEpoch[48/50] Loss: 215.960 BCE: 215.910 KL: 0.050\nEpoch[48/50] Loss: 216.520 BCE: 216.470 KL: 0.049\nEpoch[48/50] Loss: 207.925 BCE: 207.875 KL: 0.050\nEpoch[48/50] Loss: 203.150 BCE: 203.097 KL: 0.053\nEpoch[48/50] Loss: 210.638 BCE: 210.588 KL: 0.050\nEpoch[48/50] Loss: 212.494 BCE: 212.444 KL: 0.050\nEpoch[48/50] Loss: 214.399 BCE: 214.349 KL: 0.050\nEpoch[48/50] Loss: 219.334 BCE: 219.284 KL: 0.050\nEpoch[48/50] Loss: 212.070 BCE: 212.020 KL: 0.050\nEpoch[48/50] Loss: 210.169 BCE: 210.119 KL: 0.050\nEpoch[48/50] Loss: 209.553 BCE: 209.504 KL: 0.050\nEpoch[48/50] Loss: 208.826 BCE: 208.777 KL: 0.050\nEpoch[48/50] Loss: 210.536 BCE: 210.488 KL: 0.049\nEpoch[48/50] Loss: 207.446 BCE: 207.395 KL: 0.050\nEpoch[48/50] Loss: 213.504 BCE: 213.455 KL: 0.049\nEpoch[48/50] Loss: 215.153 BCE: 215.104 KL: 0.049\nEpoch[48/50] Training Loss: 210.551\nEpoch[48/50] Testing Loss: 211.107\nEpoch[49/50] Loss: 210.953 BCE: 210.904 KL: 0.050\nEpoch[49/50] Loss: 217.694 BCE: 217.646 KL: 0.048\nEpoch[49/50] Loss: 215.024 BCE: 214.974 KL: 0.050\nEpoch[49/50] Loss: 210.506 BCE: 210.457 KL: 0.048\nEpoch[49/50] Loss: 211.563 BCE: 211.513 KL: 0.050\nEpoch[49/50] Loss: 202.417 BCE: 202.366 KL: 0.051\nEpoch[49/50] Loss: 210.919 BCE: 210.870 KL: 0.049\nEpoch[49/50] Loss: 214.514 BCE: 214.464 KL: 0.050\nEpoch[49/50] Loss: 206.526 BCE: 206.476 KL: 0.050\nEpoch[49/50] Loss: 209.176 BCE: 209.126 KL: 0.050\nEpoch[49/50] Loss: 217.385 BCE: 217.336 KL: 0.049\nEpoch[49/50] Loss: 211.297 BCE: 211.247 KL: 0.050\nEpoch[49/50] Loss: 203.907 BCE: 203.857 KL: 0.050\nEpoch[49/50] Loss: 211.756 BCE: 211.707 KL: 0.049\nEpoch[49/50] Loss: 216.725 BCE: 216.676 KL: 0.049\nEpoch[49/50] Loss: 210.043 BCE: 209.995 KL: 0.049\nEpoch[49/50] Loss: 208.655 BCE: 208.606 KL: 0.049\nEpoch[49/50] Loss: 206.209 BCE: 206.159 KL: 0.049\nEpoch[49/50] Loss: 208.805 BCE: 208.756 KL: 0.049\nEpoch[49/50] Loss: 209.164 BCE: 209.115 KL: 0.049\nEpoch[49/50] Loss: 203.574 BCE: 203.524 KL: 0.049\nEpoch[49/50] Loss: 203.411 BCE: 203.361 KL: 0.050\nEpoch[49/50] Loss: 210.614 BCE: 210.565 KL: 0.049\nEpoch[49/50] Loss: 213.020 BCE: 212.972 KL: 0.048\nEpoch[49/50] Loss: 207.182 BCE: 207.132 KL: 0.050\nEpoch[49/50] Loss: 211.246 BCE: 211.197 KL: 0.049\nEpoch[49/50] Loss: 211.422 BCE: 211.373 KL: 0.049\nEpoch[49/50] Loss: 214.791 BCE: 214.743 KL: 0.048\nEpoch[49/50] Loss: 206.323 BCE: 206.273 KL: 0.049\nEpoch[49/50] Loss: 209.180 BCE: 209.131 KL: 0.049\nEpoch[49/50] Loss: 201.407 BCE: 201.357 KL: 0.050\nEpoch[49/50] Training Loss: 210.545\nEpoch[49/50] Testing Loss: 210.935\nEpoch[50/50] Loss: 211.889 BCE: 211.840 KL: 0.049\nEpoch[50/50] Loss: 209.952 BCE: 209.904 KL: 0.047\nEpoch[50/50] Loss: 206.369 BCE: 206.320 KL: 0.050\nEpoch[50/50] Loss: 212.998 BCE: 212.950 KL: 0.048\nEpoch[50/50] Loss: 208.202 BCE: 208.153 KL: 0.049\nEpoch[50/50] Loss: 209.772 BCE: 209.722 KL: 0.050\nEpoch[50/50] Loss: 205.335 BCE: 205.287 KL: 0.048\nEpoch[50/50] Loss: 203.693 BCE: 203.643 KL: 0.050\nEpoch[50/50] Loss: 202.745 BCE: 202.696 KL: 0.049\nEpoch[50/50] Loss: 217.052 BCE: 217.004 KL: 0.048\nEpoch[50/50] Loss: 208.727 BCE: 208.677 KL: 0.049\nEpoch[50/50] Loss: 206.003 BCE: 205.956 KL: 0.048\nEpoch[50/50] Loss: 210.920 BCE: 210.872 KL: 0.048\nEpoch[50/50] Loss: 216.330 BCE: 216.282 KL: 0.049\nEpoch[50/50] Loss: 209.147 BCE: 209.099 KL: 0.048\nEpoch[50/50] Loss: 210.752 BCE: 210.705 KL: 0.047\nEpoch[50/50] Loss: 217.710 BCE: 217.663 KL: 0.047\nEpoch[50/50] Loss: 214.081 BCE: 214.033 KL: 0.048\nEpoch[50/50] Loss: 217.689 BCE: 217.642 KL: 0.047\nEpoch[50/50] Loss: 204.226 BCE: 204.176 KL: 0.050\nEpoch[50/50] Loss: 203.003 BCE: 202.954 KL: 0.049\nEpoch[50/50] Loss: 212.580 BCE: 212.532 KL: 0.048\nEpoch[50/50] Loss: 204.071 BCE: 204.022 KL: 0.049\nEpoch[50/50] Loss: 212.562 BCE: 212.514 KL: 0.048\nEpoch[50/50] Loss: 213.928 BCE: 213.881 KL: 0.047\nEpoch[50/50] Loss: 219.291 BCE: 219.244 KL: 0.047\nEpoch[50/50] Loss: 208.749 BCE: 208.701 KL: 0.048\nEpoch[50/50] Loss: 218.327 BCE: 218.278 KL: 0.049\nEpoch[50/50] Loss: 209.056 BCE: 209.007 KL: 0.048\nEpoch[50/50] Loss: 215.475 BCE: 215.427 KL: 0.048\nEpoch[50/50] Loss: 221.503 BCE: 221.456 KL: 0.047\nEpoch[50/50] Training Loss: 210.556\nEpoch[50/50] Testing Loss: 211.026\n"
    }
   ],
   "source": [
    "epochs = 50\n",
    "loss_record = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    # for training...\n",
    "    for idx, (images, _) in enumerate(trainLoader):\n",
    "        images = images/255. # normalize\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "        loss, bce, kld = loss_fn(recon_images, images, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss\n",
    "\n",
    "        if idx % save_interval == 0:\n",
    "            to_print = \"Epoch[{}/{}] Loss: {:.3f} BCE: {:.3f} KL: {:.3f}\".format(epoch+1, epochs, loss.item()/batch_size, bce.item()/batch_size, kld.item()/batch_size)\n",
    "            print(to_print)\n",
    "            # sample = Variable(torch.randn(1024, 32))\n",
    "            # print(vae.fc2(sample).shape)\n",
    "            # sample = vae.decoder(vae.fc2(sample).view(32, 256, 2, 2))\n",
    "            # save_image(sample.data.view(32, 3, 64, 64), './result/Sample_' + str(epoch) + '.png')\n",
    "    \n",
    "    loss_record.append(train_loss)\n",
    "    to_print = \"Epoch[{}/{}] Training Loss: {:.3f}\".format(epoch+1,epochs, train_loss/len(trainSet))\n",
    "    print(to_print)\n",
    "    \n",
    "    # for testing... \n",
    "    for idx, (images, _) in enumerate(testLoader):\n",
    "        images = images/255. # normalize\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "        loss, _, _ = loss_fn(recon_images, images, mu, logvar)\n",
    "        test_loss += loss\n",
    "    \n",
    "    to_print = \"Epoch[{}/{}] Testing Loss: {:.3f}\".format(epoch+1,epochs, test_loss/len(testSet))\n",
    "    print(to_print)\n",
    "    \n",
    "    # when epoch finishing...\n",
    "    img = testSet[randint(1, 100)][0].unsqueeze(0)\n",
    "    compare_img = compare(img)*255\n",
    "    save_image(compare_img.data, 'sample_image.png')\n",
    "    # display(Image('sample_image.png',width=700, unconfined=True))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x16696d5a6d0>]"
     },
     "metadata": {},
     "execution_count": 170
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"259.116562pt\" version=\"1.1\" viewBox=\"0 0 378.465625 259.116562\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 259.116562 \r\nL 378.465625 259.116562 \r\nL 378.465625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 235.238437 \r\nL 371.265625 235.238437 \r\nL 371.265625 17.798437 \r\nL 36.465625 17.798437 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m20fc53a926\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m20fc53a926\" y=\"235.238437\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(48.502557 249.836875)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"113.798835\" xlink:href=\"#m20fc53a926\" y=\"235.238437\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 10 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(107.436335 249.836875)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"175.913862\" xlink:href=\"#m20fc53a926\" y=\"235.238437\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(169.551362 249.836875)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"238.02889\" xlink:href=\"#m20fc53a926\" y=\"235.238437\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 30 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(231.66639 249.836875)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"300.143918\" xlink:href=\"#m20fc53a926\" y=\"235.238437\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 40 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(293.781418 249.836875)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"362.258946\" xlink:href=\"#m20fc53a926\" y=\"235.238437\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 50 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(355.896446 249.836875)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mc887718f14\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mc887718f14\" y=\"211.985212\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 1.22 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 215.78443)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mc887718f14\" y=\"181.869454\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 1.24 -->\r\n      <g transform=\"translate(7.2 185.668672)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mc887718f14\" y=\"151.753696\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 1.26 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 155.552915)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mc887718f14\" y=\"121.637938\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 1.28 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 125.437157)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mc887718f14\" y=\"91.52218\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 1.30 -->\r\n      <g transform=\"translate(7.2 95.321399)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mc887718f14\" y=\"61.406422\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 1.32 -->\r\n      <g transform=\"translate(7.2 65.205641)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mc887718f14\" y=\"31.290664\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 1.34 -->\r\n      <g transform=\"translate(7.2 35.089883)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_14\">\r\n     <!-- 1e8 -->\r\n     <defs>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n     </defs>\r\n     <g transform=\"translate(36.465625 14.798437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-49\"/>\r\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"125.146484\" xlink:href=\"#DejaVuSans-56\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p4eb3304dc2)\" d=\"M 51.683807 27.682074 \r\nL 57.89531 144.247765 \r\nL 64.106812 169.049417 \r\nL 70.318315 177.576031 \r\nL 76.529818 183.59846 \r\nL 82.741321 186.052532 \r\nL 88.952824 190.220433 \r\nL 95.164326 192.39467 \r\nL 101.375829 193.710006 \r\nL 107.587332 196.300684 \r\nL 113.798835 197.240777 \r\nL 120.010337 199.435855 \r\nL 126.22184 201.025124 \r\nL 132.433343 201.839574 \r\nL 138.644846 204.015618 \r\nL 144.856349 205.003174 \r\nL 151.067851 206.220694 \r\nL 157.279354 207.036229 \r\nL 163.490857 208.374332 \r\nL 169.70236 209.36502 \r\nL 175.913862 210.563868 \r\nL 182.125365 211.146909 \r\nL 188.336868 212.886757 \r\nL 194.548371 212.707387 \r\nL 200.759874 214.200286 \r\nL 206.971376 214.017664 \r\nL 213.182879 215.445874 \r\nL 219.394382 215.851111 \r\nL 225.605885 216.18889 \r\nL 231.817388 217.368343 \r\nL 238.02889 218.211223 \r\nL 244.240393 218.226401 \r\nL 250.451896 218.880997 \r\nL 256.663399 219.354297 \r\nL 262.874901 219.759775 \r\nL 269.086404 220.841895 \r\nL 275.297907 221.144498 \r\nL 281.50941 220.941638 \r\nL 287.720913 221.970995 \r\nL 293.932415 222.440559 \r\nL 300.143918 222.475735 \r\nL 306.355421 223.156592 \r\nL 312.566924 223.381617 \r\nL 318.778426 223.437632 \r\nL 324.989929 224.303159 \r\nL 331.201432 224.550469 \r\nL 337.412935 224.951491 \r\nL 343.624438 225.238554 \r\nL 349.83594 225.354801 \r\nL 356.047443 225.175432 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 235.238438 \r\nL 36.465625 17.798438 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 235.238438 \r\nL 371.265625 17.798438 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 235.238437 \r\nL 371.265625 235.238437 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 17.798437 \r\nL 371.265625 17.798437 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p4eb3304dc2\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"17.798437\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(loss_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fc version\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Linear-1                 [-1, 1000]      12,289,000\n            Linear-2                   [-1, 50]          50,050\n            Linear-3                   [-1, 50]          50,050\n            Linear-4                 [-1, 1000]          51,000\n            Linear-5                [-1, 12288]      12,300,288\n================================================================\nTotal params: 24,740,388\nTrainable params: 24,740,388\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.05\nForward/backward pass size (MB): 0.11\nParams size (MB): 94.38\nEstimated Total Size (MB): 94.53\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "# fc VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # self.fc1 = nn.Linear(64*64*3, 400)\n",
    "        # self.fc2_1 = nn.Linear(400,20)\n",
    "        # self.fc2_2 = nn.Linear(400,20)\n",
    "        # self.fc3 = nn.Linear(20,400)\n",
    "        # self.fc4 = nn.Linear(400,64*64*3)\n",
    "        self.fc1 = nn.Linear(64*64*3, 1000)\n",
    "        self.fc2_1 = nn.Linear(1000,50)\n",
    "        self.fc2_2 = nn.Linear(1000,50)\n",
    "        self.fc3 = nn.Linear(50,1000)\n",
    "        self.fc4 = nn.Linear(1000,64*64*3)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc2_1(h1), self.fc2_2(h1) # mu and var\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 64*64*3))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    # Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_func(recon_x, x, mu, logvar):\n",
    "    bce_loss = F.binary_cross_entropy(recon_x, x.view(-1, 64*64*3), reduction='sum')\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return bce_loss+kl_divergence\n",
    "vae = VAE()\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(vae, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 fc\n",
    "def train(epoch):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae(data)\n",
    "        loss = loss_func(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        # if batch_idx % save_interval == 0:\n",
    "        #      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #         epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        #         100. * batch_idx / len(train_loader),\n",
    "        #         loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "def test(epoch):\n",
    "    vae.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import models\n",
    "# from torchsummary import summary\n",
    "\n",
    "# # vgg = models.vgg16()\n",
    "# summary(vae, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# for epoch in range(1, epochs + 1):\n",
    "#     train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1 cnn\n",
    "# for epoch in range(epochs):\n",
    "#     # vae.train()\n",
    "#     epoch_loss = 0\n",
    "#     for batch_idx, (data, _) in enumerate(train_loader, 0):\n",
    "#         #train network\n",
    "#         data = Variable(data)\n",
    "#         print(data.shape)\n",
    "#         optimizer.zero_grad()\n",
    "#         recon_x, mu, logvar = vae.forward(data)\n",
    "#         loss = loss_func(recon_x, data, mu, logvar)\n",
    "#         loss.backward()\n",
    "#         total_loss += loss#.data[0]\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if batch_idx % log_interval == 0:\n",
    "#             sample = Variable(torch.randn(64, latent_code))\n",
    "#             sample = vae.decoder(vae.fc2(sample).view(64, 128, 7, 7))\n",
    "#             save_image(sample.data.view(64, 1, 28, 28),'./result/mysample_' + str(epoch) + '.png')\n",
    "#             print('Train Epoch:{} -- [{}/{} ({:.0f}%)] -- Loss:{:.6f}'.format(epoch, i*len(data), len(train_loader.dataset), 100.*i/len(train_loader), loss/len(data)))\n",
    "#             print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, total_loss / len(train_loader.dataset)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit80e5783b53564d659ac8b5891bf20cfd",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}